{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finetuing LLM model to generate children stories",
   "id": "7a8a7c1517401d1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Importing Libraries",
   "id": "578cf9cccfa761b7"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T23:11:58.885134Z",
     "start_time": "2025-04-17T23:11:56.026030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/para/PycharmProjects/StoryGenerationLLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Check if CUDA is enabled",
   "id": "f9dded4c378e87b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:11:58.993834Z",
     "start_time": "2025-04-17T23:11:58.945398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"No GPU detected, using CPU\")"
   ],
   "id": "35c77e985726d27a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU detected: NVIDIA GeForce RTX 4080 SUPER\n",
      "GPU memory: 16.69 GB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:11:59.576710Z",
     "start_time": "2025-04-17T23:11:59.574629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Args\n",
    "CSV_PATH = 'data/train.csv'\n",
    "MODEL_DIR = 'models/'\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "STORIES_SAMPLE_SIZE = 100\n",
    "TRAINING_ARGS_OUTPUT_DIR = \"./training-args/mistral-stories-generator\"\n"
   ],
   "id": "5e2a62b52c804b7c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading Training dataset into pandas dataframe",
   "id": "b54710b32eb7f7d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:29.334753Z",
     "start_time": "2025-04-17T23:12:19.881003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loading dataset to pandas dataframe\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df)} stories\")\n",
    "df.columns = ['story']\n",
    "print(df.head())\n"
   ],
   "id": "f3b40e4898f7a634",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2119719 stories\n",
      "                                               story\n",
      "0  One day, a little girl named Lily found a need...\n",
      "1  Once upon a time, there was a little car named...\n",
      "2  One day, a little fish named Fin was swimming ...\n",
      "3  Once upon a time, in a land full of trees, the...\n",
      "4  Once upon a time, there was a little girl name...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preprocessing the dataset for training on Hugging Face model",
   "id": "e5ee757874c27c77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:29.396769Z",
     "start_time": "2025-04-17T23:12:29.376567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating samples\n",
    "stories_sample = df.sample(n=STORIES_SAMPLE_SIZE, random_state=42)"
   ],
   "id": "d17d99ae39da844c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:29.422096Z",
     "start_time": "2025-04-17T23:12:29.419157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Classifying stories to genre and plots using rule based solution\n",
    "genres = [\"fantasy\", \"adventure\", \"fairy tale\", \"bedtime\", \"moral\", \"magical\"]\n",
    "plot_elements = [\n",
    "    \"forest, magic\", \"castle, dragon\", \"river, talking animals\",\n",
    "    \"space, stars\", \"garden, fairies\", \"ocean, mermaids\"\n",
    "]\n",
    "\n",
    "# Create training pairs\n",
    "pairs = []\n",
    "for i, story in enumerate(stories_sample['story']):\n",
    "    genre = genres[i % len(genres)]\n",
    "    plot = plot_elements[i % len(plot_elements)]\n",
    "\n",
    "    pairs.append({\n",
    "        \"prompt\": f\"Write a bedtime story in the genre of {genre} with these elements: {plot}\\\\n\\\\n\",\n",
    "        \"completion\": story\n",
    "    })\n",
    "\n",
    "dataset_df = pd.DataFrame(pairs)"
   ],
   "id": "8d4d71da9d418c83",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:29.466004Z",
     "start_time": "2025-04-17T23:12:29.463908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_for_training(examples):\n",
    "    texts = []\n",
    "    for prompt, completion in zip(examples['prompt'], examples['completion']):\n",
    "        # Format: \"<s>[INST] {prompt} [/INST] {completion} </s>\"\n",
    "        # This follows Mistral's instruction fine-tuning format\n",
    "        text = f\"<s>[INST] {prompt} [/INST] {completion} </s>\"\n",
    "        texts.append(text)\n",
    "    return {'text': texts}"
   ],
   "id": "93409fd4d5e445bd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:29.516783Z",
     "start_time": "2025-04-17T23:12:29.507783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to HF Dataset\n",
    "dataset = Dataset.from_pandas(dataset_df)\n",
    "processed_dataset = dataset.map(\n",
    "    format_for_training,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion']\n",
    ")\n"
   ],
   "id": "1893c34b46ea3cab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 44715.39 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:29.609222Z",
     "start_time": "2025-04-17T23:12:29.607604Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a5d1c4d2b3c05a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:31.898953Z",
     "start_time": "2025-04-17T23:12:31.394667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "id": "400afc12c6a7821",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:31.904144Z",
     "start_time": "2025-04-17T23:12:31.902122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048  # Adjust based on your stories' length\n",
    "    )"
   ],
   "id": "e76fe49b50f36421",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:32.296148Z",
     "start_time": "2025-04-17T23:12:32.230977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ],
   "id": "1c5010a6aaa96d4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2636.57 examples/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:33.489760Z",
     "start_time": "2025-04-17T23:12:33.484930Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)",
   "id": "895989e3235660f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:34.371505Z",
     "start_time": "2025-04-17T23:12:34.369558Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import BitsAndBytesConfig",
   "id": "f2957a65318f6a8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:34.843320Z",
     "start_time": "2025-04-17T23:12:34.840470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ],
   "id": "50a2a7b4d98073d9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:44.685944Z",
     "start_time": "2025-04-17T23:12:35.337858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config\n",
    ")\n"
   ],
   "id": "a42ba9987ed8d7c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:44.690879Z",
     "start_time": "2025-04-17T23:12:44.688857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ],
   "id": "8b17489027c5b1d4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:44.902976Z",
     "start_time": "2025-04-17T23:12:44.733067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ],
   "id": "95be5a33a9f9866c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 7,261,655,040 || trainable%: 0.1877\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:45.644378Z",
     "start_time": "2025-04-17T23:12:45.642342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ],
   "id": "b46d3fed9d2329e8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:47.095616Z",
     "start_time": "2025-04-17T23:12:47.011514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=TRAINING_ARGS_OUTPUT_DIR,\n",
    "    eval_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,  # Increase this to simulate larger batch sizes\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Set to \"wandb\" if you want to use Weights & Biases\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ],
   "id": "45615601146913d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:12:48.518268Z",
     "start_time": "2025-04-17T23:12:48.506930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ],
   "id": "2591cf125051ec4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20596/3030834965.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:21:34.371082Z",
     "start_time": "2025-04-17T23:12:49.378945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nStarting training...\")\n",
    "trainer.train()"
   ],
   "id": "d5034850b1fb2973",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/para/PycharmProjects/StoryGenerationLLM/.venv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 08:35, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=66, training_loss=1.6191459785808215, metrics={'train_runtime': 524.8609, 'train_samples_per_second': 0.514, 'train_steps_per_second': 0.126, 'total_flos': 2.277130691936256e+16, 'train_loss': 1.6191459785808215, 'epoch': 2.888888888888889})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:21:51.033010Z",
     "start_time": "2025-04-17T23:21:51.030954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "now = datetime.now()\n",
    "formatted_timestamp = now.strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ],
   "id": "d02f6c8fe3d4c4f5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:21:54.361665Z",
     "start_time": "2025-04-17T23:21:53.954267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.save_model(MODEL_DIR + f\"mistral-stories-{STORIES_SAMPLE_SIZE}stories-{formatted_timestamp}\")\n",
    "print(\"\\nModel saved\")"
   ],
   "id": "733e2fa132dd891d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:22:38.057865Z",
     "start_time": "2025-04-17T23:22:38.054897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_story(genre, plot, max_length=1000):\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"<s>[INST] Generate a children's story in the genre of {genre} with the following plot: {plot} [/INST]\"\n",
    "\n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode and return\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt part from the output\n",
    "    story = story.replace(prompt, \"\").strip()\n",
    "    return story"
   ],
   "id": "bc513af84d31746c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:23:32.034058Z",
     "start_time": "2025-04-17T23:22:40.138793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_genre = \"adventure\"\n",
    "test_plot = \"A young explorer discovers a magical map leading to a hidden island\"\n",
    "print(f\"\\nGenerating a test story in the genre '{test_genre}' with the plot: '{test_plot}'\")\n",
    "story = generate_story(test_genre, test_plot)\n",
    "print(\"\\nGenerated Story:\")\n",
    "print(story)"
   ],
   "id": "83eecc1d38d12653",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating a test story in the genre 'adventure' with the plot: 'A young explorer discovers a magical map leading to a hidden island'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/para/PycharmProjects/StoryGenerationLLM/.venv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/para/PycharmProjects/StoryGenerationLLM/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Story:\n",
      "Generate a children's story in the genre of adventure with the following plot: A young explorer discovers a magical map leading to a hidden island  Once upon a time, there was a little boy named Timmy. Timmy was three years old and loved to explore. One day, he was exploring the backyard when he found something shiny. It was a silver key! Timmy was so excited. He ran inside to tell his mom about the key. \n",
      "\n",
      "\"Mommy, look what I found!\" Timmy said.\n",
      "\n",
      "\"Oh, a shiny key! What does it do?\" Timmy's mom asked.\n",
      "\n",
      "\"I don't know, but I want to find out!\" Timmy said.\n",
      "\n",
      "Timmy's mom smiled and said, \"Let's see if we can find something it fits.\"\n",
      "\n",
      "They looked around the house and found a small wooden box. The key fit perfectly into the box. Timmy was so proud of himself. He had found something special. \n",
      "\n",
      "From that day on, Timmy loved to explore and find new things. He always carried the silver key with him, just in case he found something special. \n",
      "\n",
      "The End. \n",
      "\n",
      "Timmy and his mom were so excited to see what the key would unlock. They went from room to room, trying the key in different places. They looked in cupboards, drawers, and even in the fridge. But nothing seemed to fit. \n",
      "\n",
      "After a while, Timmy's mom said, \"Maybe it doesn't fit anything here. Maybe it's meant for somewhere else.\"\n",
      "\n",
      "Timmy nodded and kept the key in his pocket. He continued to explore the house, hoping he would find something that the key would fit. \n",
      "\n",
      "After a few days, Timmy's mom found a small, locked box in the attic. She took the key out and said, \"I think this is what the key is for.\"\n",
      "\n",
      "She opened the box and inside was a beautiful necklace. Timmy's eyes lit up and he said, \"Wow, I found something special!\" \n",
      "\n",
      "Timmy's mom smiled and said, \"Yes, you did. It's a family heirloom that has been passed down for generations. You should be very proud.\" \n",
      "\n",
      "From that day on, Timmy was even more proud of the silver key. He knew it had unlocked something very special. \n",
      "\n",
      "The End. \n",
      "\n",
      "Timmy and his mom continued to explore the house. They found lots of new things and Timmy was so happy. But he still couldn't find anything that the silver key would fit. \n",
      "\n",
      "After a while, Timmy's mom said, \"Maybe the key doesn't fit anything in our house. Maybe it's meant for something else.\"\n",
      "\n",
      "Timmy nodded and kept the key in his pocket. He continued to explore the house, hoping he would find something that the key would fit. \n",
      "\n",
      "After a few days, Timmy's mom found a small, locked box in the attic. She took the key out and said, \"I think this is what the key is for.\"\n",
      "\n",
      "She opened the box and inside was a beautiful necklace. Timmy's eyes lit up and he said, \"Wow, I found something special!\"\n",
      "\n",
      "Timmy's mom smiled and said, \"Yes, you did. It's a family heirloom that has been passed down for generations. You should be very proud.\"\n",
      "\n",
      "From that day on, Timmy was even more proud of the silver key. He knew it had unlocked something very special. \n",
      "\n",
      "The End. \n",
      "\n",
      "Timmy and his mom continued to explore the house. They found lots of new things and Timmy was so happy. But he still couldn't find anything that the silver key would fit. \n",
      "\n",
      "After a while, Timmy's mom said, \"Maybe the key doesn't fit anything in our house. Maybe it's meant for something else.\"\n",
      "\n",
      "Timmy nodded and kept the key in his pocket. He continued to explore the house, hoping he would find something that the key would fit. \n",
      "\n",
      "After a few days, Timmy's mom found a small, locked box in the attic. She took the key out and said, \"I think this is what the key is for.\"\n",
      "\n",
      "She opened the box and inside was a beautiful necklace. Timmy's eyes lit up and he said, \"Wow, I found something special!\"\n",
      "\n",
      "Timmy's mom smiled and said, \"Yes, you did. It's a family heirlo\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c0dac263d9efcb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
